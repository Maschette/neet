---
title: "When is done *done*? Practice fundamentals of automated testing for collaborative data analysis development"
subtitle: "How to go from no tests, to *whoa* tests!"
author: "sketch-draft by lead author, Charles T. Gray"
bibliography: references.bib
output: 
  html_document:
    number_sections: true
    code_folding: hide
# output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Version of this manuscript

This manuscript will go through three phases:

1. A lead author sketch draft.
2. Implementation with the reproducibiliity team for [The repliCATS Project](https://replicats.research.unimelb.edu.au/) & version listing any team members who wish to co-author. 
3. Implementation of repliCATS procedures, explicitly, after we need not blind the algorithms for experimental ingegrity. Invitation for external co-authors to contribute and extend the content, review, and final submission. Possibly for useR2020. 

# Coding to doneness 

Contrapunctus I is rarely learnt in one sitting, indeed, for most pianists, to even play the piece is the result of many months' diligent work. For the student of this opening work of Bach's *The Art of Fugue*, the time at the keyboard spent to play through a three-minute piece of music can seem immeasurable. To play well, a diligent pianist must employ technqiue. But *how* a pianist is to achieve the desired technique is not clear [@chang_fundamentalspianopractice_2009a]. 

For a researcher performing an analysis using a computational tool such as R, it can be unclear when an analysis is *done*. And, while there are technical guides on *good enough* practice, it can feel overwhelming as to what to adopt and where. Particularly for an in-development algorithm, which is to say, an algorithm with scripts already started, and some results explored. 

This manuscript picks up from where Riederer left off with *RMarkdown-driven development* and suggests a  *coderegistering test-driven workflow* for coding to doneness. This workflow provides a roadmap to completion for a packaged analysis with `code::proof`ed the manuscript [@gray2019textttcodeproof], provided measures of confidence in the implementaiton of the algorithm.   

Musicians and, similarly, athletes, do not see themselves as having mastered a skill, but active practitioners of a craft [@galway_flute_1990]. To be a flautist is to practice, and to be athlete is to train. 

# Questionable research practices in scientific computing

Algorithms are coded by people who practice code, and significant problems emerge when algorithms are treated as fixed artifacts, rather than one of the tools utilised by those who *practice* code. In Australia, a heartbreakingly-ongoing example of this kind of problem with the income-reporting data analyis algorithm that assesses if income welfare recipients have been overpaid entitlements due to working. Crude averaging calculations have lead to ongoing incorrect debt notices issued, such as 20,000 people receiving "robodebt" notices for debts they did not owe [@mcilroy201720]. Problems in data analysis have real impacts on real people's lives. Perhaps, if the algorithm were considered a workflow practiced and monitored by a team of data scientists, rather than a static object, problems would not be persisting to this day [@karp_robodebtfederalcourt_2019]. 

We might view this as a computational instantiation of what Fraser *et al.* denote *questionable research practices* (QRPs). The QRPs Fraser *et al.* provide a taxonomy for refer to various practices in scientific methodology found in ecology and evolution, such as $p$-hacking, adjusting a model's specification to achieve a desired $p$-value, or *cherry picking*, failing to report variables or results that do not fit a desired narrative. QRPs are, importantly, often not a result of scientific malpractice, but a question of discipline-specific conventions established in bygone eras not translating well to data-driven research [@fraser_questionable_2018]. In scientific computing, as the robodebt example illustrates, similar and overlapping errors may occur. 

A consideration when providing recommendations of best practice is what me might reasonably expect of a researcher. Indeed, it is likely unrealistic to expect *best practices* in scientific computing [@wilson_best_2014], perhaps we would be better off asking for *good enough* practices [@wilson_good_2017] in scientific computing. For while researchers use computational algorithms in science every day, most of them are not trained in computational science. Even mathematicians and statisticians do a great deal of training at the blackboard, rather than the computer.    

# Practice fundamentals


Key arguments to keep in mind:

- two key arguments: workflow towards doneness and also effective collaboration
- how to practice testing? test-driven development
- *not* what is best-practice
- *not* comprehensive guide to testing
- collaborative --> anxiety reducing
- in-dev testing scripts are gold
- preregistration & qrps


# Older stuff

# Fundamentals of practice

Chuang C. Chang's _Fundamentals of Piano Practice_ sets out to address a gap in piano pedagogy [@chang_fundamentalspianopractice_2009a]. A similar gap exists in the implementation of *good enough* practices [@wilson_good_2017], which is to say, what we might reasonably expect of a analyst, in reproducible computing for research data analyses. The objective is different from advanced pianism, but we characterise testing *practice*, as opposed to *techique*, analogous to how Change delineates between *piano* practice and technique. Through attempting to identify the fundamentals of automated testing for collaborative data analysis development, this manuscript aims to articulate the gap in understanding automated testing implementation for analysts. 

Testing as described in *R Packages* [todo: cite] and the primers provided by RStudio [todo: cite],   

- what does change mean by 

As Chang notes, whilst there a rich history of technical pedagogy, there is a dearth of guidance for pianists on _learning_ the technique [@chang_fundamentalspianopractice_2009a]. There are many canonical texts of pianistic technique pedagogy. Bach provides a pathway from small canons [todo: cite], to two-part inventions[todo: cite], three-part sinfonia, and the challenge of *The Well-Tempered Clavier* and *The Art of Fugue*. Bartok provides the *Mikrokosmos* [todo: ], and Czerny's *School of Velocity* [to do]. In each case, technical exercises of increasing difficulty are provided. In piano pedagogy, a _technical_ exercise isolates a particular aspect of pianistic technique [todo examples]. For example, [todo Czerny staccato]. Or, [todo voicing technique].

The dearth that Chang attempts to address is in pianistic _practice_ habits that will lead to succesful adoption of these techniques. In science, we might call this _work flow_ [todo cite]. 

> .. practically every piano learning method consists of showing students what to practice, and what kinds of techniques (runs, arpeggios, legato, staccato, trills, etc.) are needed. There are few instructions on how to practice order  to be able to play them, which is mostly left to the student and endless repetitions [@chang_fundamentalspianopractice_2009a].

Wilson et al. followed their work on _best practices_  in scientific computing [@wilson_best_2014], with a reflection on _good enough_ practices [@wilson_good_2017], in recognition that we must account for what we might reasonably request of practitioners of data analysis. In this manuscript, we consider one component of *good enough* practice in data analysis: *automated testing*. 

Automated tests are a formalised way of implementing checks that inputs and outputs of algorithsms are as expected [@wickham_r_2015]. Informative messages are output that assist the developer in identifying where code is not performing as expected. 

## Collaboration via automated testing 

At heart, automated tests are collaborative. This may be with others, but applies at least as much to a analyst approaching their own past work with which they have become unfamiliar, or have become anxious about some aspect of the work. Automated tests provide an efficient way of returning to and extending an analysis; anxiety is reduced by having defined outcomes to code explicitly for. 

Reproducible research compendia provide a means by which analysts can share their work so that it may be extended by others, and automated tests provide `code::proof` [@grayCodeProofPrepare2019], a measure of confidence in the algorithm for others. Hayes expresses concern about using algorithms published without automated tests [@hayes_testing_2019]. However, only one quarter of the largest repository of R packages, [The Comprehensive R Archive Network](https://cran.r-project.org/), have any automated tests [@hester_covr_2016], highlighting that, despite testing identified as a 'vital' part of data analysis [@wickham_r_2015], automated testing is yet to be widely adopted.

Indeed, as noted in Wilson's testing primers (in development) for [RStudio](https://rstudio.cloud/learn/primers) [@_rstudiocloud_], 

> Almost all (92%) of the catastrophic system failures are the result of incorrect handling of non-fatal errors explicitly signaled in software. In 58% of the catastrophic failures, the underlying faults could easily have been detected through simple testing of error handling code [@yuan_simpletestingcan_2014].

Scientific errors that are inadvertently introduced by faulty workflow are characterised in metascience as *questionable research practices*.

## Questionable research practices in scientific computing

One way to characterise automated tests is to consider them part of a suite of methodologies that are being developed to mitigate _questionable research practices_ [@fraser_questionable_2018]. For social scientists and ecologists, a literature is emerging on how our methodologies inadvertently lead to errors in scientific reasoning, $p$-hacking, altering a statistical model to achieve a desired outcome. But there are others, too, *HARKing*, hypothesising after the fact, where the question is changed to achieve a desired outcome from the model, and *cherry picking*, leaving out troublesome aspects of the analysis that don't fit a desired narrative. 

Gould is extending these ideas to an examination of the workflow of ecological models, adapting Gelman and [todo]  

[todo: gould extension bridge to computing]

As researcher software engineers, it behoves us to consider what are questionable research practices in software produced for data analyses. Version control and open code sharing via a platform such as GitHub, is one way to mitigate questionable research practices in scientific computing [@Bryan2017ExcuseMD]. There is also a growing literature on reproducible research compendia via packaged analyses [@marwick_packaging_2018; @wilson_good_2017].     

This manuscript contributes to this literature by focussing on workflows for automated tests that assist the developer communicate what they have coded to others and their future self, as a means of mitigating questionable research practices in scientific computing. 

## Test-driven development

- test-drive development mitigates 

## The benefits of preregistring code

> inherently collaborative

# A test-driven toolchain walkthrough for an in-development analysis

In this section we now consider the practicality of implementing tests through a *toolchain walkthrough*, an opinionated documetation of a scientific workflow , towards a measure of `code::proof`, confidence in the algorithm implemented [@grayCodeProofPrepare2019]. In particular, a toolchain walkthrough is a reflection of *one* workflow, whilst others, indeed, better, workflows might exist. This is in contrast to a comprehensive review of tools. Instead, a toolchain walkthrough ruminates on considerations *after* a set of tools have been chosen.

Toolchain walkthroughs aim to identify obstacles and advantages in implementation of scientific workflows. By necessity, a toolchain walkthrough is language specific, but, ideally, observations emerge that are useful for any appropriate language employed for data analysis, such as Python. This toolchain walkthrough examines the *process*, analogous to piano *practice*, of implementing tests, as opposed to defining comprehensively the nature of *good enough* tests, analogous to guidance on pianistic technique.

## Objective of this toolchain walkthrough

This toolchain walkthrough aims to provide guidance on implementing a test-driven workflow for an in-development analysis. Many analyses begin as scripts that develop [todo expan] [@riederer_rmarkdowndrivendevelopment_2019]. The central question of this manuscript is what constitutes a minimal level of testing for in-development analysis, where code is still being written and features implemented. Automated tests assist in time-consuming debugging tasks [@wickham_r_2015], but also in providing information with a developer who is unfamiliar with the code. 

This is a first effort in identifying the fundamentals of automated testing for the collaborative process of developing an analysis in R. Analgous to Riederer's *RMarkdown-driven development* [@riederer_rmarkdowndrivendevelopment_2019], which deconstructs the workflow of developing an analysis from .Rmd notebook-style reports to packaged analyses, we consider a set a computational tools that form a workflow to assist in the coherent development of automated tests for data analysis. This is an extension of the workflow suggestions provided in *R Packages*, with a specific focus on collaborative workflows in research.   

## Devops and assumed expertise

This toolchain walkthrough assumes a knowledge of R advanced enough to be using the language to be answering scientific research claims.

- tools used: testthat, neet, covr
- GitHub


## Get an overview with `covr::`

With a packaged analysis, it's easy to get a sense of existing automated testing coverage, if any tests have been written. The analysis functions provided by `varameta::` were put on hold while signficant discussion was written.   

``` r
library(covr)
package_coverage("~/Documents/repos/varameta/")
#> varameta Coverage: 70.71%
#> R/dist_name.R: 0.00%
#> R/g_cauchy.R: 44.44%
#> R/g_norm.R: 71.43%
#> R/hozo_se.R: 92.31%
#> R/bland_mean.R: 100.00%
#> R/bland_se.R: 100.00%
#> R/effect_se.R: 100.00%
#> R/g_exp.R: 100.00%
#> R/g_lnorm.R: 100.00%
#> R/hozo_mean.R: 100.00%
#> R/wan_mean_C1.R: 100.00%
#> R/wan_mean_C2.R: 100.00%
#> R/wan_mean_C3.R: 100.00%
#> R/wan_se_C1.R: 100.00%
#> R/wan_se_C2.R: 100.00%
#> R/wan_se_C3.R: 100.00%
```

<sup>Created on 2020-02-01 by the [reprex package](https://reprex.tidyverse.org) (v0.3.0)</sup>

The `covr::` function `package_coverage` provides the percentage of the lines of code run in tests. 

# A toolchain walkthrough for using automated tests `varameta::`

## Code with intent

When we think of an algorithm, it's easy to feel overwhelmed by the complexity. Here are the relationships between inputs, and just some of the estimators, and outputs in `varameta`, presented as a randomised *graph*, a visual representation of a set of nodes, $V$, and the edges, $V \times V$, between them.

```{r message=FALSE}
library(igraph)

confusing_relationships <- make_graph(~ 
a-+hozo, b-+hozo, m-+hozo, n-+hozo, hozo-+s, hozo-+mean,
a-+wan1, b-+wan1, m-+wan1, n-+wan1, wan1-+s, wan1-+mean,
a-+pren, b-+pren, m-+pren, n-+pren, pren-+s,
q1-+wan3, q3-+wan3, m-+wan3, n-+wan3, wan3-+s, wan3-+s,
q1-+pren, q3-+pren, m-+pren, n-+pren, pren-+s,
a-+wan2, b-+wan2, q1-+wan2, q3-+wan2, m-+wan2, n-+wan2, wan2-+s, wan2-+mean,
a-+bland, b-+bland, q1-+bland, q3-+bland, m-+bland, n-+bland, bland-+s, bland-+mean
           ) 

confusing_relationships %>% plot(
             vertex.color = "grey",
             vertex.frame.color = "darkgrey",
             vertex.size = 10,
             vertex.label.cex = 0.8,
             edge.arrow.size = 0.4
             )

```



The Center for Open Science recommend *preregistering* an experiment, stating what the hypothesis is and how the analyst intends to assess the hypothesis as one safeguard against inadvertent questionable research practices. Analogously, we might think of preregistering our code as providing some `code::proof` of the implementation of our algorithm. In this manuscript, we will think of *coderegistration*, stating what the intention of an algorithm is, as 

In the 2000s, Francesca Cann taught concert-pianists in training to _practice with intent_, to do what one has intended to do. 

## Coderegistering an algorithm

For this coderegistration, we  To coderegister an algorithm:
  
### On a piece of paper.
  
1. Draw a diagram of the inputs and outputs of the algorithm.
2. Draw where this algorithm fits in the pipeline of the package, if appropriate.

### In an issue on GitHub:

1. Describe the algorithm's intended purpose.
2. Describe the input parameters and how they will be tested.
3. Describe the output parameters and how they will be tested.

Update the coderegistration as needed. 

### Sketch-diagram 

An image or diagram is very help, if not burdonsomely time consuming (but it often is).

For those comfortable with *graphs* understood as mathematical objects as a set of vertices from $V$, and a set of edges $V \times V$. There are visualisation optionss where the vertices can be tagged with attributes.  

```{r message=FALSE}
library(igraph)

confusing_relationships <- make_graph(~ 
a-+hozo, b-+hozo, m-+hozo, n-+hozo, hozo-+s, hozo-+mean,
a-+wan1, b-+wan1, m-+wan1, n-+wan1, wan1-+s, wan1-+mean,
a-+pren, b-+pren, m-+pren, n-+pren, pren-+s,
q1-+wan3, q3-+wan3, m-+wan3, n-+wan3, wan3-+s, wan3-+s,
q1-+pren, q3-+pren, m-+pren, n-+pren, pren-+s,
a-+wan2, b-+wan2, q1-+wan2, q3-+wan2, m-+wan2, n-+wan2, wan2-+s, wan2-+mean,
a-+bland, b-+bland, q1-+bland, q3-+bland, m-+bland, n-+bland, bland-+s, bland-+mean
           ) 

confusing_relationships %>% plot(
             vertex.color = "grey",
             vertex.frame.color = "darkgrey",
             vertex.size = 10,
             vertex.label.cex = 0.8,
             edge.arrow.size = 0.4
             )

```


- ggplot picture of algorithm? use vis thingy from drake?



```{r message=FALSE, fig.width=15, fig.cap="Codebrain before coderegistration."}
library(tidyverse)
library(ggraph)
library(tidygraph)

nodes <- tribble(
  ~node, ~state,
  "a", "input",
  "b", "input",
  "q1", "input",
  "q3", "input",
    "m", "input",
  "n","input",
  "hozo", "estimator",
  "pren_c3", "estimator",
  "pren_c1", "estimator",
  "bland", "estimator",
  "wan_c1", "estimator",
  "wan_c2", "estimator",
  "wan_c3", "estimator",
  "se_median", "output",
  "mean", "output",
  "sd", "output",
  "g_cauchy_c1", "estimator",
  "g_cauchy_c3", "estimator",
  "g_exp_c1", "estimator",
  "g_exp_c3", "estimator",
  "g_norm_c1", "estimator",
  "g_norm_c3", "estimator",
  "g_lnorm_c1", "estimator",
  "g_lnorm_c3", "estimator"
) %>% 
  mutate(state = fct_relevel(state, "input"))

edges <- tribble(
  ~from, ~to,
  # hozo estimator
  5,7, 
  6,7,
  1,7,
  2,7,
  7,15,
  7,16,

  # wan1 estimator
  5,11, 
  6,11,
  1,11,
  2,11,
  11,15,
  11,16,
  
  # pren1 estimator
  5,9, 
  6,9,
  1,9,
  2,9,
  9,14,
  
  # bland estimator
  5,10,
  6,10,
  1,10,
  2,10,
  3,10,
  4,10,
  10,15,
  10,16,
  
  # wan2 estimator
  5,12,
  6,12,
  1,12,
  2,12,
  3,12,
  4,12,
  12,15,
  12,16,

  # wan3 estimator
  5,13, 
  6,13,
  3,13,
  4,13,
  13,15,
  13,16,

  # pren3
  5,8, 
  6,8,
  3,8,
  4,8,
  8,14,

  # g_cauchy_c1
  5,17, 
  6,17,
  1,17,
  2,17,
  17,14,
  
  # g_cauchy_c3
  5,18, 
  6,18,
  3,18,
  4,18,
  18,14,

  # g_exp_c1
  5,19, 
  6,19,
  1,19,
  2,19,
  19,14,
  
  # g_exp_c3
  5,20, 
  6,20,
  3,20,
  4,20,
  20,14,

  # g_norm_c1
  5,21, 
  6,21,
  1,21,
  2,21,
  21,14,
  
  # g_norm_c3
  5,22, 
  6,22,
  3,22,
  4,22,
  22,14,
  
  # g_lnorm_c1
  5,23, 
  6,23,
  1,23,
  2,23,
  23,14,
  
  # g_lnorm_c3
  5,24, 
  6,24,
  3,24,
  4,24,
  24,14
  
)

graph <- tbl_graph(nodes, edges)

graph %>% 
  mutate(state = fct_relevel(state, "output")) %>% 
  ggraph() +
  geom_edge_link(arrow = arrow(), colour = "lightgrey") + 
  geom_node_label(aes(label = node, colour = state),  
                  size = 5,
                  fill = "lightgrey",
                  alpha = 0.6) +
  theme_graph() +
  hrbrthemes::scale_color_ipsum() +
  theme(legend.position = "none") + 
  scale_y_reverse() + 
  coord_flip() 
  
```


## `neet` test the inputs and outputs

## `neet` test all input cases

##  complete tests for 


### identifying the testable components


## Workflow for in-development analysis

- If not neet than neet. If neet, then what?

# Collaboration

# Assessing the doneness of software 

# Further directions

# References
