---
title: "When is done *done*? Fundamentals of automated testing for collaborative data analysis development"
author: "Charles T. Gray, Daniel Fryer, Elise Gould, and Aaron todo"
bibliography: references.bib
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Fundamentals

In Chuang C. Chang's _Fundamentals of Piano Practice_ the author sets out to address a gap in piano pedagogy [@chang_fundamentalspianopractice_2009a]. A similar gap exists in the implementation of *good enough* practices [@wilson_good_2017], which is to say, what we might reasonably expect of a analyst, in reproducible computing for research data analyses. The objective is different from advanced pianism, however, in this manuscript we shall chracterise this gap by considering automated testing workflow in research data analysis through piano pedgogy. Through attempting to identify the fundamentals of automated testing for collaborative data analysis development, this manuscript aims to articulate the gap in understanding automated testing implementation for analysts. 

As Chang notes, whilst there a rich history of technical pedagogy, there is a dearth of guidance for pianists on _learning_ the technique [@chang_fundamentalspianopractice_2009a]. There are many canonical texts of pianistic technique pedagogy. Bach provides a pathway from small canons [todo: cite], to two-part inventions[todo: cite], three-part sinfonia, and the challenge of *The Well-Tempered Clavier* and *The Art of Fugue*. Bartok provides the *Mikrokosmos* [todo: ], and Czerny's *School of Velocity* [to do]. In each case technical exercises of increasing difficulty are provided. In piano pedagogy, a _technical_ exercise isolates a particular aspect of pianistic technique [todo examples]. For example, [todo Czerny staccato]. Or, [todo voicing technique].

The dearth that Chang attempts to address is in pianistic _practice_ habits that will lead to succesful adoption of these techniques. In science, we might call this _work flow_ [todo cite]. 

> .. practically every piano learning method consists of showing students what to practice, and what kinds of techniques (runs, arpeggios, legato, staccato, trills, etc.) are needed. There are few instructions on how to practice order  to be able to play them, which is mostly left to the student and endless repetitions [@chang_fundamentalspianopractice_2009a].

Wilson et al. followed their work on _best practices_  in scientific computing [@wilson_best_2014], with a reflection on _good enough_ practices [@wilson_good_2017], in recognition that we must account for what we might reasonably request of practitioners of data analysis. In this manuscript, we consider one component of *good enough* practice in data analysis: *automated testing*. 

Automated tests are a formalised way of implementing checks that inputs and outputs of algorithsms are as expected [@wickham_r_2015]. Informative messages are output that assist the developer in identifying where code is not performing as expected. 

## Collaboration via automated testing 

At heart, automated tests are collaborative. This may be with others, but applies at least as much to a analyst approaching their own past work with which they have become unfamiliar. Automated tests provide an efficient way of returning to and extending an analysis. 

Reproducible research compendia provide a means by which analysts can share their work so that it may be extended by others, and automated tests provide `code::proof` [@grayCodeProofPrepare2019], a measure of confidence in the algorithm for others. Hayes expresses concern about using algorithms published without automated tests [@hayes_testing_2019]. However, only one quarter of the largest repository of R packages, [The Comprehensive R Archive Network](https://cran.r-project.org/), have any automated tests [@hester_covr_2016], highlighting that, despite testing identified as a 'vital' part of data analysis [@wickham_r_2015], automated testing is yet to be widely adopted.

Indeed, as noted in Wilson's testing primers (in development) for [RStudio](https://rstudio.cloud/learn/primers) [@_rstudiocloud_], 

> Almost all (92%) of the catastrophic system failures are the result of incorrect handling of non-fatal errors explicitly signaled in software. In 58% of the catastrophic failures, the underlying faults could easily have been detected through simple testing of error handling code [@yuan_simpletestingcan_2014].

Scientific errors that are inadvertently introduced by faulty workflow are characterised in metascience as *questionable research practices*.

## Questionable research practices in scientific computing

One way to characterise automated tests is to consider them part of a suite of methodologies that are being developed to mitigate _questionable research practices_ [@fraser_questionable_2018]. For social scientists and ecologists, a literature is emerging on how our methodologies inadvertently lead to errors in scientific reasoning, $p$-hacking, altering a statistical model to achieve a desired outcome. But there are others, too, *HARKing*, hypothesising after the fact, where the question is changed to achieve a desired outcome from the model, and *cherry picking*, leaving out troublesome aspects of the analysis that don't fit a desired narrative. 

Gould is extending these ideas to an examination of the workflow of ecological models.

[todo: gould extension bridge to computing]



# References


